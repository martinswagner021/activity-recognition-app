Future<void> _runInference() async {
    if (!_modelLoaded || _dataWindow.length < windowSize) {
      print("Model not loaded or data window not full.");
      return;
    }

    // --- 1. Preprocess the input data ---
    // The input shape depends on how your model was trained.
    // Assuming [1, windowSize, numFeatures] for raw time series.
    // Flatten the list of lists into a single list for the Float32List
    var flatInputList = _dataWindow.expand((sublist) => sublist).toList();
    var inputBytes = Float32List.fromList(flatInputList);
    

    // Reshape to what the model expects, e.g., [1, windowSize, numFeatures]
    // The actual shape needs to match your model's input tensor details.
    // Use the details printed in _loadModel() to confirm.
    // Let's assume input tensor shape is [1, 128, 3]
    var inputTensorShape = _interpreter.getInputTensor(0).shape;
    if (inputTensorShape.length == 3 &&
        inputTensorShape[0] == 1 &&
        inputTensorShape[1] == windowSize &&
        inputTensorShape[2] == numFeatures) {
      // Input is already effectively shaped by how Float32List is created from a flat list
      // and how the interpreter expects it.
    } else if (inputTensorShape.length == 2 && inputTensorShape[0] == 1) {
        // This might be the case if your model expects a flattened list of features
        // or if windowSize * numFeatures equals inputTensorShape[1].
        // Example: inputTensorShape = [1, 384] for windowSize=128, numFeatures=3
        if (inputTensorShape[1] != windowSize * numFeatures) {
             print("Model input shape mismatch! Expected [1, ${windowSize * numFeatures}], got ${inputTensorShape}");
             return;
        }
    }
    else {
        print("Model input shape mismatch! Expected [1, $windowSize, $numFeatures] or [1, ${windowSize * numFeatures}], got ${inputTensorShape}");
        return;
    }

    // You might need to reshape the inputBytes if the interpreter doesn't handle the flat list directly
    // for a 3D tensor. Often, tflite_flutter handles this if the total number of elements matches.
    // For explicit reshaping:
    // var reshapedInput = inputBytes.buffer.asFloat32List().reshape([1, windowSize, numFeatures]);
    // This reshape method isn't directly on Float32List, you'd typically prepare the list of lists
    // and then create the final input structure.
    // For now, we'll assume the flat list is okay if the total elements match.

    // --- 2. Prepare output buffer ---
    // The output shape depends on your model.
    // Assuming it's [1, num_classes] for classification.
    var outputTensorShape = _interpreter.getOutputTensor(0).shape; // e.g., [1, 6]
    if (outputTensorShape.length != 2 || outputTensorShape[0] != 1 || outputTensorShape[1] != _activityLabels.length) {
        print("Model output shape mismatch! Expected [1, ${_activityLabels.length}], got ${outputTensorShape}");
        // If the output shape is different, you MUST adjust _activityLabels and how you process output.
        // return; // Commenting out to allow trying with potential mismatch
    }
    // The size of the output buffer should match the number of elements in the output tensor.
    int outputSize = outputTensorShape.fold(1, (a, b) => a * b); // Calculate total elements
    var outputBuffer = List.filled(outputSize, 0.0).reshape(outputTensorShape);


    // --- 3. Run inference ---
    try {
      // The input to interpreter.run should be a List<Object> or an Object.
      // If your model has one input, it's `[inputBytesList]` or `inputBytesList.buffer`
      // If your input tensor is e.g. [1, 128, 3], you need to provide a List<List<List<double>>>
      // or a flat Float32List that the interpreter can map to this shape.
      // Let's try providing the _dataWindow directly, assuming tflite_flutter can handle List<List<double>>
      // if the shape matches. Or, provide the flattened Float32List.

      // Option A: Using the original _dataWindow (List<List<double>>) wrapped in another list for batch=1
      // This assumes the interpreter can map List<List<double>> to a [1, windowSize, numFeatures] tensor
      // if the input tensor is defined as such.
      var inputForModel = [_dataWindow]; // Becomes [1, windowSize, numFeatures]

      // Option B: Using the flattened Float32List.
      // This is often more robust if the model expects a flat buffer that it internally reshapes.
      // var inputForModel = inputBytes.buffer; // Pass the ByteBuffer
      // Or, if the interpreter expects a List<Object> where each Object is an input tensor:
      // var inputForModel = [inputBytes.buffer.asFloat32List()]; // This might be needed

      // Let's check the input tensor type. If it's FLOAT32, Float32List is good.
      // The structure (nested lists vs flat list) depends on how tflite_flutter maps it.
      // The `_interpreter.run(inputs, outputs)` method expects:
      // inputs: A List<Object>, where each Object is an input tensor.
      // outputs: A Map<int, Object>, where the key is the output tensor index and Object is the buffer.

      // We'll use the flattened list for input, which is common.
      // The interpreter will map this flat list to the input tensor's shape.
      Map<int, Object> outputs = {0: outputBuffer}; // Output tensor at index 0
      _interpreter.runForMultipleInputs([inputBytes], outputs); // Pass input as List<Object>

      // --- 4. Post-process the output ---
      // outputBuffer now contains the prediction.
      // If outputBuffer is List<List<double>> (e.g. [[prob1, prob2, ...]]), take the first element.
      List<double> probabilities;
      if (outputBuffer is List && outputBuffer.isNotEmpty && outputBuffer[0] is List) {
        probabilities = (outputBuffer[0] as List).map((e) => e as double).toList();
      } else if (outputBuffer is List) { // If outputBuffer is already a flat list of probabilities
        probabilities = (outputBuffer as List).map((e) => e as double).toList();
      }
      else {
        print("Unexpected outputBuffer format: ${outputBuffer.runtimeType}");
        return;
      }


      // Find the index with the highest probability
      double maxProb = 0.0;
      int predictedIndex = -1;
      for (int i = 0; i < probabilities.length; i++) {
        if (probabilities[i] > maxProb) {
          maxProb = probabilities[i];
          predictedIndex = i;
        }
      }

      setState(() {
        if (predictedIndex != -1 && predictedIndex < _activityLabels.length) {
          _predictedActivity = _activityLabels[predictedIndex];
        } else if (predictedIndex != -1) {
            _predictedActivity = "Unknown Label (Index: $predictedIndex)";
        }
        else {
          _predictedActivity = "No prediction";
        }
        _outputProbabilities = probabilities; // Store for display
      });

    } catch (e) {
      print("Error running inference: $e");
      setState(() {
        _predictedActivity = "Error in inference.";
      });
    }
  }